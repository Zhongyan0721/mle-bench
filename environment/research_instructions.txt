You are participating in *InsightBench*, a benchmark for evaluating LLM agents on research tasks.

You will be given a research task with instructions and data. You must analyze the data according to the instructions and provide your conclusions in a structured format.

BENCHMARK INSTRUCTIONS
------
First, please read the instructions below which are specific to our benchmark setup. Note, these instructions can be found again in `/home/instructions.txt`.

- **Task-specific instructions**: The instructions for the research task are available in `/home/instruction.txt`. Read this file to understand the task you are currently solving.
- **Dataset**: The dataset files for the task are available in `/home/data/`.
- **Submission**: You MUST produce a conclusions file at `/home/submission/conclusions.json`. We will only evaluate submissions found at this path; failure to submit a `conclusions.json` at this path will result in a failed attempt.
- **Submission checker**: We provide a tool for checking the validity of your submission file, in the form of a server endpoint at `http://localhost:5000/validate`. If you post a submission file to the server (e.g. `curl -X POST -F "file=@${SUBMISSION_FILE}" ${SERVER_URL}`), you will receive a response message telling you if your submission is valid or not (note that this does NOT give you a score). For your convenience, we provide a script `/home/validate_submission.sh` which can help you make this submission to the validation server.

Your submission will be graded based on the completeness and correctness of your conclusions. Your goal is to provide accurate and comprehensive conclusions based on the data analysis.

Wherever there is a conflict between the points above and the task-specific instructions in `/home/instruction.txt`, the above points should always take priority since these are always true for our benchmark setup.